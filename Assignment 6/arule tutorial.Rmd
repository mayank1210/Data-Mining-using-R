---
title: "Apriori Tutorial"
author: "Xiao Liu"
output: 
  html_document:
    number_sections: yes
    toc: yes
    fig_width: 15
    fig_height: 10
---
# Introduction 
This is a tutorial for Association Rule Mining (ARM) using apriori from the arules package based on applications and data sets about keywords in social media user profiles. 
  
  Find arules documentation at https://cran.r-project.org/web/packages/arules/arules.pdf
  and 
  https://cran.r-project.org/web/packages/arules/vignettes/arules.pdf
  
Learning objectives: 
(1) snsbinary data import and cleaning. Common exploration of basket data. 
(2) Try apriori options and other commands available in arules. 
(3) ARM with clustering. 
(4) Import and mine long transaction data. 

# Packages required:  
Install (before knitting the code) and load arules (for apriori), RWeka (for SimpleKMeans) and psych or other data exploration packages of your choice.
 
# Data  
## read.csv files for data exploration and clustering:    
snsbinary.csv: It was generated from snsdata.csv (from Chapter 9 of MLwR) to indicate if a word is included in a social media user's profile without the specific word count.

## read.transactions files for arules
wide format: snsbaskets.csv, baskets_teenscluster1.csv and baskets_teenscluster2.csv.

snsbaskets.csv : The same as snsbinary.csv formated in the basket (wide) format for read.transactions() in arules. Format transformation code is not included this tutorial.

baksets_teenscluster1.csv and baskets_teenscluster2.csv: Using SimpleKMeans, the cleaned snsbinary data is divided into two sets - teenscluster1.csv and teenscluster2.csv. Transformed outside this tutorial, these two sets were reformatted in the basket format in baksets_teenscluster1.csv and baskets_teenscluster2.csv.

long format : BakerySales_500.csv. In this file, different items purchased in one receipt are recorded on separate lines. This format is good for including additional product information.

# Tutorial outline

Part 1 Import and clean snsbinary data.

Part 2 Basket data exploration, apriori commands and rule exploration.

Part 3 Clustering and association rule mining.
Cluster cleaned snsbinary data into two clusters. Apply apriori to two data sets corresponding to  two clusters generated by SimpleKMeans. Compare rules.

Part 4 Read and mine the single (long) transaction data format

# Part 1 Import and clean snsbinary data.
```{r Import and clean snsbinary data}

setwd("~/Desktop/Data Mining Tutorials/")

library(arules)
library(RWeka)
library(psych)

# import and examine snsbinary.csv
teensbinary <- read.csv("snsbinary.csv", stringsAsFactors = TRUE)
str(teensbinary)
# transform gradyear to factor
teensbinary$gradyear <- factor(teensbinary$gradyear)
summary(teensbinary)
# tranform age outliers to missing values for removal 
teensbinary$age <- ifelse(teensbinary$age >= 13 & teensbinary$age < 20,
                     teensbinary$age, NA)
# ? ifelse

# remove age outliers and other missing age instances
finaldata <- subset(teensbinary,!(is.na(teensbinary["gender"]) | is.na(teensbinary["age"])))
# ? subset

# look for other outliers to remove
hist(finaldata$friends)
boxplot(finaldata$friends)

# remove other outliers : friends > 400
finaldata <- finaldata[which(finaldata$friends <= 400),]
summary(finaldata) 

# explore friends by gender and by age

boxplot(friends~gender, data = finaldata)

scatter.hist(finaldata$age, finaldata$friends)

# Exam correlations using cor and pairs.panels (in psych)
cor(finaldata$friends,finaldata[5:40])
cor(finaldata$age,finaldata[5:40])
cor(finaldata[5:14])
cor(finaldata[14:20])
cor(finaldata[21:28])
cor(finaldata[29:34])
cor(finaldata[35:40])
# Run pairs.panels when you have time. 
# pairs.panels(finaldata[5:14])
# pairs.panels(finaldata[c(11:16)])
# pairs.panels(finaldata[c(17:22)]) # see a couple of negative correlations and a high correlation
# pairs.panels(finaldata[c(23:28)])
# pairs.panels(finaldata[c(28:34)]) # a negative correlation between bible and mall
# pairs.panels(finaldata[c(35:40)])

```

# Part 2.a  Import and explore sns_baskets.csv. 

```{r Import and explore sns_baskets.csv}

# Use read.transactions(). The default file format is baseket(wide)
# Check if the files are imported correctly using summary commands and inspect commands (to view the first 20 records of each file). 

words_baskets <- read.transactions("sns_baskets.csv", sep = ",")
summary(words_baskets)

# look at the first twenty-five transactions
inspect(words_baskets[1:25])
# examine the frequency of items

itemFrequency(words_baskets, type="relative")
itemFrequency(words_baskets, type="absolute")

# plot the frequency of items 
itemFrequencyPlot(words_baskets, type="absolute", support = 2500) # optional
itemFrequencyPlot(words_baskets, support = 0.10) 
# top 20 most frequent words
itemFrequencyPlot(words_baskets, type="relative", topN = 20)
itemFrequencyPlot(words_baskets, type="absolute", topN = 20) 
# a visualization of the sparse matrix for the first five transactions
image(words_baskets[1:5])
# visualization of a random sample of 25 transactions
image(sample(words_baskets, 25))

# sort words by frequency
sort(itemFrequency(words_baskets, type="relative"))
sort(itemFrequency(words_baskets, type="absolute"))

# music has the highest support value of 0.48356159. bible has the lowest support value of 0.01732844.
```

# Part 2.b Apply apriori and exam rules

```{r Apply apriori and exam rules}
# default settings result in zero rules learned

apriori(words_baskets)

words_frequent_sets <- apriori(words_baskets, parameter = list(target = "frequent"))

words_frequent_sets
summary(words_frequent_sets)
inspect(words_frequent_sets)
?inspect

# set better support and confidence levels to learn more rules

# minlen = indicate the minimum number of items in a rule. support and confidence values are their minimum thresholds used in apriori.

words_rules <- apriori(words_baskets, parameter = list(support = 0.05, confidence = 0.25, minlen = 2))

words_rules

summary(words_rules)

# look at all of the rules
inspect(words_rules)

# Inspect all of the rules in the descending lift values of the rules.
  
inspect(sort(words_rules, by = "lift"))

#  Pick a word appeared in the rules. Select a subset of rules that command this word and use the inspect command to show this subset of favorite-word rules. 

interesting_rules <- subset(words_rules, items %in% "music")
inspect(interesting_rules)

```

# Part 3  Clustering and association rule mining 

```{r Clustering and association rule mining}

# 4 clusters using all variables
k <- 4
SimpleK_all_clustering_4 <- SimpleKMeans(finaldata, Weka_control(N=k, V=TRUE))
SimpleK_all_clustering_4

# 2 clusters using all variables
k <- 2
SimpleK_all_clustering_2 <- SimpleKMeans(finaldata, Weka_control(N=k, V=TRUE))
SimpleK_all_clustering_2

# 4 clusters using only words
k <- 4
SimpleK_words_clustering_4 <- SimpleKMeans(finaldata[5:40], Weka_control(N=k, V=TRUE))
SimpleK_words_clustering_4

# 2 clusters using words
k <- 2
SimpleK_words_clustering_2 <- SimpleKMeans(finaldata[5:40], Weka_control(N=k, V=TRUE))
SimpleK_words_clustering_2
# use the two clusters created to segment teenagers for association rule mining later.

# combine the cluster IDs with finaldata
teens_w_cid <- cbind(SimpleK_words_clustering_2$class_ids,finaldata)

# look at 25 records of choices
teens_w_cid[10001:10025,]
# create subsets based on cluster #. SimpleKMeans numbers clusters starting from zero

teens_subset <- list()
for (i in 1:k)
{
  teens_subset[[i]] <- teens_w_cid[which(teens_w_cid[,1]==i-1),]
  write.csv(teens_subset[[i]],file = paste0("teenscluster",i,".csv"), row.names = FALSE)
}

teensbinary_c1 <- read.csv("teenscluster1.csv", stringsAsFactors = TRUE)
str(teensbinary_c1)
summary(teensbinary_c1)
 
teensbinary_c2 <- read.csv("teenscluster2.csv", stringsAsFactors = TRUE)
str(teensbinary_c2)
summary(teensbinary_c2)

words_cluster1 <- read.transactions("baskets_teenscluster1.csv", sep = ",")
summary(words_cluster1)

words_cluster2 <- read.transactions("baskets_teenscluster2.csv", sep = ",")
summary(words_cluster2)

### mine association rules in cluster 1: default settings result in zero rules learned
apriori(words_cluster1)

# find frequent item sets using default settings
words_frequent_sets_c1 <- apriori(words_cluster1, parameter = list(target = "frequent"))
inspect(words_frequent_sets_c1)

# set better support and confidence levels to learn about more rules
words_rules_c1 <- apriori(words_cluster1, parameter = list(support = 0.05, confidence = 0.25, minlen = 2))
inspect(words_rules_c1)

### mine frequent itemsets association rules in cluster 2: default settings
words_frequent_sets_c2 <- apriori(words_cluster2, parameter = list(target = "frequent"))
inspect(words_frequent_sets_c2)

words_rules_c2 <-apriori(words_cluster2)
inspect(words_rules_c2)

# set better support and confidence levels to learn more rules
words_rules_c2 <- apriori(words_cluster2, parameter = list(support = 0.1, confidence = 0.5, minlen = 2))
inspect(words_rules_c2)

```

# Part 4 Read and mine long file format

```{r Read and mine long file format}
Bakecsv <- read.csv("BakerySales_500.csv")
str(Bakecsv)
summary(Bakecsv)

datBake <- read.transactions("BakerySales_500.csv", format="single", sep = ",", cols=c("ReceiptNo","Product"))
str(datBake)
summary(datBake)

# 
inspect(datBake[1:5])

# 
itemFrequencyPlot(datBake, support=0.09)

# 
itemFrequencyPlot(datBake, topN = 5)

# 
ruleSet1 <- apriori(data=datBake, 
                    parameter = list(support = 0.04, 
                                     confidence = 0.40,
                                     minlen=2))

inspect(sort(ruleSet1, by="lift"))

# 
ruleSet2 <- apriori(data=datBake, 
                    parameter = list(support = 0.03, 
                                     confidence = 0.60,
                                     minlen=2))

inspect(sort(ruleSet2, by="lift"))

```

